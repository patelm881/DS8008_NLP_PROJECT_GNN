{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "The following notebook contains the implementation for the LSTM network. The code and experiments ran on the selected datasets is shown in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Meghna\n",
      "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import shuffle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import zeros\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prepare_data function takes the name of the desired input file to be processed, and returns the processed data (X), the processed labels (y), the word-index pairs (word2idx), the length of the longest sentence (maxLen) and total number of unique labels (totalTags). The purpose of this function is to read the input file, and process it for input into the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_data):\n",
    "    # read the input data\n",
    "    if (input_data == 'subjectivity'):\n",
    "        data = pd.read_csv(\"data/subjectivity.txt\", delimiter = \"\\t\", \n",
    "                           header = None, names=['tag','sentence'])\n",
    "    \n",
    "    elif (input_data == 'mpqa'):\n",
    "        data = pd.read_csv(\"data/mpqa.txt\", delimiter = \"\\t\", \n",
    "                           header = None, encoding='latin-1',names=['tag','sentence'])\n",
    "        \n",
    "    elif (input_data == 'bbc'):\n",
    "        data = pd.read_csv(\"data/bbc_text.txt\", delimiter = \"\\t\", \n",
    "                           header = None, names=['tag','sentence'])\n",
    "    \n",
    "    elif (input_data == 'rt-polarity'):\n",
    "        data = pd.read_csv(\"data/rt-polarity.txt\", delimiter = \"\\t\", \n",
    "                           header = None, encoding='latin-1',names=['tag','sentence'])\n",
    "        \n",
    "    df = pd.DataFrame(columns = ['Sentence#', 'Word', 'Tag']) # define an empty dataframe \n",
    "    \n",
    "    # tokenize word and pair with its corresonding tag, store this information in a df \n",
    "    for id, sent in data.iterrows():\n",
    "        tokens=[word.lower() for word in nltk.word_tokenize(sent[1])]\n",
    "        for tk in tokens:\n",
    "            sid = 'Sentence:'+str(id) \n",
    "            new_row = {'Sentence#': sid, 'Word': tk, 'Tag': sent[0]}\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # build a word to index and tag to index list \n",
    "    words = list(set(df['Word'].values))\n",
    "    words.append('UNK')\n",
    "    totalWords = len(words)\n",
    "\n",
    "    tags = list(set(df[\"Tag\"].values))\n",
    "    totalTags = len(tags)\n",
    "    agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),s[\"Tag\"].values.tolist())]\n",
    "    sentencesData = df.groupby(\"Sentence#\").apply(agg_func)\n",
    "    sentencesData=[s for s in sentencesData]\n",
    "\n",
    "    largest_sen = max([len(sen) for sen in sentencesData])\n",
    "    maxLen = largest_sen\n",
    "    word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "    tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "    print(tag2idx)\n",
    "    \n",
    "    # make all sentences equal size by adding the UNK token at the end of each sentences whose size is less\n",
    "    # than maximum sentence lenght\n",
    "    \n",
    "    X = [[word2idx[w[0]] for w in s] for s in sentencesData]\n",
    "    X = pad_sequences(maxlen=maxLen, sequences=X, padding=\"post\", value=word2idx['UNK'])#totalWords)\n",
    "    Y = [[tag2idx[w[1]] for w in s] for s in sentencesData]\n",
    "    \n",
    "    # additional padding specific to the datasets \n",
    "    if input_data == 'bbc':\n",
    "        Y = pad_sequences(maxlen=maxLen, sequences=Y, padding=\"post\", value=tag2idx['sport'])\n",
    "    elif input_data == 'mpqa':\n",
    "        Y = pad_sequences(maxlen=maxLen, sequences=Y, padding=\"post\", value=tag2idx[0])\n",
    "    else:\n",
    "        Y = pad_sequences(maxlen=maxLen, sequences=Y, padding=\"post\", value=tag2idx['pos'])\n",
    "    Y = [to_categorical(tagIdx, num_classes=totalTags) for tagIdx in Y]\n",
    "    y = np.array(Y)\n",
    "    \n",
    "    return X, y, word2idx, maxLen, totalTags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function builds the embedding matrix using pre-trained Google News vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(word2idx):\n",
    "    embeddings_index = dict()\n",
    "    # Reading embedding file\n",
    "    f = open('GoogleNews-vectors-negative300.bin','rb') # load the pre-trained Google News vectors \n",
    "    # iterate the file line by line add words and coefficient accordingly\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:])\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    vocab_size = len(word2idx) + 1\n",
    "    embedding_matrix = zeros((vocab_size, 300)) # initialize an empty embedding matrix \n",
    "    for word, idx_word in word2idx.items():\n",
    "        embedding_vector = embeddings_index.get(word.lower()) \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx_word] = embedding_vector\n",
    "    input = Input(shape=(maxLen,))\n",
    "    \n",
    "    return embedding_matrix, vocab_size, input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define_lstm_model is a helper function to define the lstm model. This function takes as input the vocabulary size, the embeddings matrix (defined using pre-trained Google News vectors), the maximum length (input length), total tags, input and data. For the activation function we use softmax activation. We use adam as the optimization algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_lstm_model(vocab_size, embedding_matrix, maxLen, totalTags, input):\n",
    "    model = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxLen, trainable=False)(input)  \n",
    "    model = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(model)  \n",
    "    out = TimeDistributed(Dense(totalTags, activation=\"softmax\"))(model)\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_lstm is a helper function that trains the model using 10-fold cross validation. At each iter, we reserve 10% of the data as validation set. We keep track of the classification accuracy on the test set for each iter and return the mean (average) classification accuracy at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm(model, X, y):\n",
    "    kf = KFold(n_splits=10)\n",
    "    kf.get_n_splits(X)\n",
    "    KFold(n_splits=10, random_state=None, shuffle=False)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X): # 10-fold CV, build train-test set according to split indices   \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # train the model, keep 10% of the training-set as the validation set \n",
    "        history = model.fit(X_train, y_train, batch_size=32, epochs=2, validation_split=0.1, verbose=1)\n",
    "        scores.append(model.evaluate(X_test,y_test)) # keep track of accuracy for each CV iter \n",
    "    return np.mean(scores, axis = 0)[1] # return average (mean) accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on datasets\n",
    "\n",
    "In this section we run our experiements on the selected datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subjectivity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/subjectivity.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-78f4f55df63c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxLen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalTags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'subjectivity'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# prepare the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# {'neg': 0, 'pos': 1}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-98b0c189b1ac>\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(input_data)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# read the input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'subjectivity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         data = pd.read_csv(\"data/subjectivity.txt\", delimiter = \"\\t\", \n\u001b[0m\u001b[0;32m      5\u001b[0m                            header = None, names=['tag','sentence'])\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/subjectivity.txt'"
     ]
    }
   ],
   "source": [
    "X, Y, word2idx, maxLen, totalTags = prepare_data('subjectivity') # prepare the dataset \n",
    "# {'neg': 0, 'pos': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, vocab_size, input = build_embedding_matrix(word2idx) # build embedding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_lstm_model(vocab_size, embedding_matrix, maxLen, totalTags, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "acc = run_lstm(model,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean classification accuracy for the Subjectivity dataset using LSTM is: 87.66 %\n"
     ]
    }
   ],
   "source": [
    "print('The mean classification accuracy for the Subjectivity dataset using LSTM is: ' , np.round(acc*100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rt-polarity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0, 'pos': 1}\n"
     ]
    }
   ],
   "source": [
    "X, Y, word2idx, maxLen, totalTags = prepare_data('rt-polarity') # prepare the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, vocab_size, input = build_embedding_matrix(word2idx) # build embedding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_lstm_model(vocab_size, embedding_matrix, maxLen, totalTags, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "acc = run_lstm(model,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean classification accuracy for the rt-polarity dataset using LSTM is: 79.73%\n"
     ]
    }
   ],
   "source": [
    "print('The mean classification accuracy for the rt-polarity dataset using LSTM is: ' , np.round(acc*100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
